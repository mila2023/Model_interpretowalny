{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57f4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, brier_score_loss, log_loss\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d093bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"zbiór_10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1afd3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=[\"default\"])\n",
    "y = data[\"default\"]\n",
    "\n",
    "# dropujemy szczegolna forma wlasnosci (kazdy ma taka sama 117)\n",
    "X = X.drop(columns=\"szczegolnaFormaPrawna_Symbol\")\n",
    "\n",
    "unique_values = X['formaWlasnosci_Symbol'].unique()\n",
    "\n",
    "categorical_cols = ['formaWlasnosci_Symbol']\n",
    "\n",
    "# lista kolumn OHE odpowiadających symbolom form własności\n",
    "numeric_cols = [\n",
    "    'ohe_fw_214','ohe_fw_215','ohe_fw_113','ohe_fw_216','ohe_fw_225','ohe_fw_226',\n",
    "    'ohe_fw_224','ohe_fw_227','ohe_fw_234','ohe_fw_111','ohe_fw_112','ohe_fw_235',\n",
    "    'ohe_fw_132','ohe_fw_123','ohe_fw_133','ohe_fw_122','ohe_fw_338', 'ohe_fw_000'\n",
    "]\n",
    "\n",
    "# inicjalizacja OneHotEncoder z ustalonymi kategoriami\n",
    "ohe = OneHotEncoder(\n",
    "    categories=[sorted([int(c.split('_')[-1]) for c in numeric_cols])],\n",
    "    sparse_output=False,  # zmiana z sparse -> sparse_output\n",
    "    drop=None\n",
    ")\n",
    "\n",
    "# dopasowanie i transformacja\n",
    "ohe_array = ohe.fit_transform(X[['formaWlasnosci_Symbol']])\n",
    "\n",
    "# utworzenie DataFrame z odpowiednimi nazwami kolumn\n",
    "df_ohe = pd.DataFrame(ohe_array, columns=numeric_cols, index=X.index)\n",
    "\n",
    "# połączenie z oryginalnym df\n",
    "df = pd.concat([X, df_ohe], axis=1)\n",
    "df = df.drop(columns=[\"formaWlasnosci_Symbol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce27f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## analogicznie dla kolumny 'schemat_wsk_bilans'\n",
    "\n",
    "ohe_cols = ['SFJIN_wsk_bilans', 'SFJMI_wsk_bilans', 'SFJMA_wsk_bilans']\n",
    "\n",
    "# inicjalizacja OneHotEncoder z ustalonymi kategoriami\n",
    "ohe = OneHotEncoder(\n",
    "    categories = [['SFJIN', 'SFJMI', 'SFJMA']],\n",
    "    sparse_output=False,  # zmiana z sparse -> sparse_output\n",
    "    drop=None\n",
    ")\n",
    "\n",
    "# dopasowanie i transformacja\n",
    "ohe_array = ohe.fit_transform(X[['schemat_wsk_bilans']])\n",
    "\n",
    "# utworzenie DataFrame z odpowiednimi nazwami kolumn\n",
    "df_ohe = pd.DataFrame(ohe_array, columns=ohe_cols, index=X.index)\n",
    "\n",
    "# połączenie z oryginalnym df\n",
    "df = pd.concat([df, df_ohe], axis=1)\n",
    "df = df.drop(columns=[\"schemat_wsk_bilans\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "123519f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## i jeszcze raz dla 'schemat_wsk_rzis'\n",
    "\n",
    "ohe_cols = ['SFJIN_wsk_rzis', 'SFJMI_wsk_rzis', 'SFJMA_wsk_rzis']\n",
    "\n",
    "# inicjalizacja OneHotEncoder z ustalonymi kategoriami\n",
    "ohe = OneHotEncoder(\n",
    "    categories = [['SFJIN', 'SFJMI', 'SFJMA']],\n",
    "    sparse_output=False,  # zmiana z sparse -> sparse_output\n",
    "    drop=None\n",
    ")\n",
    "\n",
    "# dopasowanie i transformacja\n",
    "ohe_array = ohe.fit_transform(X[['schemat_wsk_rzis']])\n",
    "\n",
    "# utworzenie DataFrame z odpowiednimi nazwami kolumn\n",
    "df_ohe = pd.DataFrame(ohe_array, columns=ohe_cols, index=X.index)\n",
    "\n",
    "# połączenie z oryginalnym df\n",
    "df = pd.concat([df, df_ohe], axis=1)\n",
    "df = df.drop(columns=[\"schemat_wsk_rzis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b118f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "## zamiana poprzedniego sposobu liczenia woe na kawalek  pipeline'u  \n",
    "\n",
    "class PKDKodWoEEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, top_n=10, smoothing=0.5):\n",
    "        self.top_n = top_n\n",
    "        self.smoothing = smoothing\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = X.copy()\n",
    "        y = y.copy()\n",
    "        \n",
    "        # wybieramy top_n najczęstszych kategorii\n",
    "\n",
    "        self.top_values_ = X['pkdKod'].value_counts().nlargest(self.top_n).index\n",
    "        \n",
    "        # kolumna w ktorej rzadkie wartosci zamieniamy na '0'\n",
    "        grouped = X['pkdKod'].where(X['pkdKod'].isin(self.top_values_), other='0')\n",
    "        \n",
    "        # liczymy woe\n",
    "        df = pd.DataFrame({'group': grouped, 'target': y})\n",
    "        agg = df.groupby('group')['target'].agg(['sum', 'count'])\n",
    "        agg = agg.rename(columns={'sum':'bad', 'count':'total'})\n",
    "        agg['good'] = agg['total'] - agg['bad']\n",
    "\n",
    "        agg['bad_s'] = agg['bad'] + self.smoothing\n",
    "        agg['good_s'] = agg['good'] + self.smoothing\n",
    "\n",
    "        total_bad = agg['bad_s'].sum()\n",
    "        total_good = agg['good_s'].sum()\n",
    "\n",
    "        agg['woe'] = np.log((agg['good_s'] / total_good) / (agg['bad_s'] / total_bad))\n",
    "\n",
    "        self.woe_map_ = agg['woe'].to_dict()\n",
    "        self.fallback_ = np.mean(list(self.woe_map_.values()))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        grouped = X['pkdKod'].where(X['pkdKod'].isin(self.top_values_), other='0')\n",
    "        X['WoE_pkdKod_grouped'] = grouped.map(self.woe_map_).fillna(self.fallback_)\n",
    "        return X.drop(columns=['pkdKod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b0ec861",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingValueIndicatorAndImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy=\"median\"):\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.replace([np.inf, -np.inf], np.nan).copy()\n",
    "        self.base_cols_ = list(X.columns)\n",
    "\n",
    "        # kolumny do imputacji\n",
    "        self.imputer_ = SimpleImputer(strategy=self.strategy)\n",
    "        self.imputer_.fit(X[self.base_cols_])\n",
    "\n",
    "        # nazwy kolumn wskaźników\n",
    "        self.indicator_cols_ = [f\"{c}_mial_braki_danych\" for c in self.base_cols_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.replace([np.inf, -np.inf], np.nan).copy()\n",
    "\n",
    "        # imputacja\n",
    "        X_imputed = pd.DataFrame(\n",
    "            self.imputer_.transform(X[self.base_cols_]),\n",
    "            columns=self.base_cols_,\n",
    "            index=X.index\n",
    "        )\n",
    "\n",
    "        # wskaźniki braków danych\n",
    "        indicator_df = X[self.base_cols_].isna().astype(int)\n",
    "        indicator_df.columns = self.indicator_cols_\n",
    "        indicator_df.index = X.index\n",
    "\n",
    "        # łączymy razem\n",
    "        X_out = pd.concat([X_imputed, indicator_df], axis=1)\n",
    "\n",
    "        return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d5bb7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropConstantColumns(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # znajdź kolumny, które mają tylko jedną unikalną wartość\n",
    "        self.cols_to_drop_ = [col for col in X.columns if X[col].nunique() <= 1]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.cols_to_drop_, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e80fd53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrelationBasedFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.9):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = X.copy()\n",
    "        y = y.copy()\n",
    "\n",
    "        # Liczymy macierz korelacji\n",
    "        corr_matrix = X.corr().abs()\n",
    "\n",
    "        # Bierzemy tylko górny trójkąt\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "        to_drop = []\n",
    "\n",
    "        # Iterujemy po parach kolumn, które przekraczają próg korelacji\n",
    "        for col_a in upper.columns:\n",
    "            # Znajdź kolumny silnie skorelowane z col_a\n",
    "            highly_corr = upper.index[upper[col_a] > self.threshold].tolist()\n",
    "\n",
    "            for col_b in highly_corr:\n",
    "                # Jeśli żadna z kolumn jeszcze nie została usunięta\n",
    "                if col_a not in to_drop and col_b not in to_drop:\n",
    "                    \n",
    "                    # Korelacja każdej z targetem\n",
    "                    corr_a = abs(np.corrcoef(X[col_a], y)[0,1])\n",
    "                    corr_b = abs(np.corrcoef(X[col_b], y)[0,1])\n",
    "\n",
    "                    # Wywalamy tę słabiej skorelowaną z targetem\n",
    "                    if corr_a < corr_b:\n",
    "                        to_drop.append(col_a)\n",
    "                    else:\n",
    "                        to_drop.append(col_b)\n",
    "\n",
    "        self.to_drop_ = to_drop\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        return X.drop(columns=self.to_drop_, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e64fc876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline same transformacje (sprawdzamy czy robi to samo co poprzednia abominacja xddd)\n",
    "\n",
    "pipeline_transform = Pipeline([\n",
    "    (\"pkd_woe\", PKDKodWoEEncoder(top_n=10, smoothing=0.5)),\n",
    "    (\"missing\", MissingValueIndicatorAndImputer(strategy=\"median\")),\n",
    "    (\"drop_constant\", DropConstantColumns()),\n",
    "    (\"corr_selector\", CorrelationBasedFeatureSelector(threshold= 0.9))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53450d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.3, random_state=2137)\n",
    "\n",
    "X_train_pipe = pipeline_transform.fit_transform(X_train.copy(), y_train.copy())\n",
    "X_test_pipe  = pipeline_transform.transform(X_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5ee7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best params from CV: {'classifier__C': 10, 'classifier__class_weight': None, 'classifier__penalty': 'l1'}\n",
      "Best CV score (ROC AUC): 0.6697408765652524\n"
     ]
    }
   ],
   "source": [
    "## POD ZADNYM POZOREM NIE CZEKAJ AZ TO SIE ZROBI TO CHOLERSTWO KOSZTOWALO PONAD GODZINE MOJEGO ZYCIA\n",
    "## GOTOWY MODEL JEST NA GITHUBIE!\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"pkd_woe\", PKDKodWoEEncoder(top_n=10, smoothing=0.5)),\n",
    "    (\"missing\", MissingValueIndicatorAndImputer(strategy=\"median\")),\n",
    "    (\"drop_constant\", DropConstantColumns()),\n",
    "    (\"corr_selector\", CorrelationBasedFeatureSelector(threshold=0.8)),\n",
    "    (\"scaler\", StandardScaler()),  # WAŻNE dla regresji!\n",
    "    (\"classifier\", LogisticRegression(\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=1000,\n",
    "        solver='liblinear'  # dobry dla mniejszych danych\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"classifier__C\": [0.001, 0.01, 0.1, 1, 10, 100],  # siła regularyzacji\n",
    "    \"classifier__penalty\": [\"l1\", \"l2\"],  # L1 dla selekcji cech, L2 dla stabilności\n",
    "    \"classifier__class_weight\": [None, \"balanced\", {0: 1, 1: 3}, {0: 1, 1: 5}]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# UŻYJ ROC AUC - najlepsze dla nierównowagi\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=cv, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a0496fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params from CV: {'classifier__C': 10, 'classifier__class_weight': None, 'classifier__penalty': 'l1'}\n",
      "Best CV score (ROC AUC): 0.6697408765652524\n"
     ]
    }
   ],
   "source": [
    "# wczytanie modelu\n",
    "loaded_grid = joblib.load('grid_search_model.pkl')\n",
    "\n",
    "# najlepszy model\n",
    "best_model = loaded_grid.best_estimator_\n",
    "\n",
    "print(\"Best params from CV:\", grid.best_params_)\n",
    "print(\"Best CV score (ROC AUC):\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02e83aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCENA JAKOŚCI MODELU ===\n",
      "ROC AUC: 0.6850\n",
      "PR AUC: 0.1271\n",
      "Log Loss: 0.3838\n",
      "Brier Score: 0.0592\n",
      "KS Statistic: 0.3224\n"
     ]
    }
   ],
   "source": [
    "## metryki\n",
    "\n",
    "# Predykcje na zbiorze testowym\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Wszystkie wymagane metryki\n",
    "print(\"=== OCENA JAKOŚCI MODELU ===\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "print(f\"PR AUC: {average_precision_score(y_test, y_pred_proba):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_pred_proba):.4f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "# Statystyka KS\n",
    "def calculate_ks_score(y_true, y_pred_proba):\n",
    "    scores_0 = y_pred_proba[y_true == 0]\n",
    "    scores_1 = y_pred_proba[y_true == 1]\n",
    "    ks_stat, _ = ks_2samp(scores_1, scores_0)\n",
    "    return ks_stat\n",
    "\n",
    "ks_stat = calculate_ks_score(y_test, y_pred_proba)\n",
    "print(f\"KS Statistic: {ks_stat:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e804b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(best_model\u001b[38;5;241m.\u001b[39mcoef_[\u001b[38;5;241m0\u001b[39m]))]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Analiza współczynników\u001b[39;00m\n\u001b[0;32m      4\u001b[0m coefficients \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mcoef_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "# Pobieranie nazw cech po preprocessing\n",
    "feature_names = []\n",
    "# Dostosuj do swojego preprocessora - przykład:\n",
    "# if hasattr(preprocessor, 'get_feature_names_out'):\n",
    "#     feature_names = preprocessor.get_feature_names_out()\n",
    "# else:\n",
    "#     feature_names = [f'feature_{i}' for i in range(len(best_model.coef_[0]))]\n",
    "\n",
    "# Analiza współczynników\n",
    "coefficients = best_model.coef_[0]\n",
    "odds_ratios = np.exp(coefficients)\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefficients,\n",
    "    'odds_ratio': odds_ratios,\n",
    "    'abs_importance': np.abs(coefficients)\n",
    "}).sort_values('abs_importance', ascending=False)\n",
    "\n",
    "print(\"\\n=== INTERPRETACJA GLOBALNA ===\")\n",
    "print(\"Top 10 najważniejszych cech:\")\n",
    "print(feature_importance_df.head(10))\n",
    "\n",
    "# Wizualizacja współczynników\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance_df.head(15)\n",
    "colors = ['red' if coef < 0 else 'blue' for coef in top_features['coefficient']]\n",
    "plt.barh(top_features['feature'], top_features['coefficient'], color=colors)\n",
    "plt.xlabel('Współczynnik (log-odds)')\n",
    "plt.title('Top 15 najważniejszych cech - współczynniki regresji logistycznej')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_coefficients.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
